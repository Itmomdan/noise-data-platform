{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ed6a62",
   "metadata": {},
   "source": [
    "# WAV_motorcycle_MySQL_적재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ec7462",
   "metadata": {},
   "source": [
    "## 0. Spark Session 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0aa6070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# MySQL JDBC 드라이버 경로 (압축 푼 드라이버 JAR 파일 경로)\n",
    "mysql_driver_path = \"/home/ubuntu/mysql-connector-j-9.2.0/mysql-connector-j-9.2.0.jar\"\n",
    "\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WAV_motor\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://localhost:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .config(\"spark.jars\", mysql_driver_path) \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b4cf36",
   "metadata": {},
   "source": [
    "## 1. wav_motor_horn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738987d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import io\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "# HDFS에서 모든 WAV 파일 읽기\n",
    "hdfs_dir = \"hdfs://localhost:9000/shared_data/raw_data/2.Motorcycle/4.horn_of_motorcycle\"\n",
    "binary_df = spark.read.format(\"binaryFile\").load(hdfs_dir)\n",
    "\n",
    "# 🔹 UDF (User Defined Function) 정의: WAV → MFCC 변환\n",
    "def extract_mfcc(binary_data):\n",
    "    try:\n",
    "        audio_bytes = io.BytesIO(binary_data)  # 바이너리 데이터를 메모리 파일로 변환\n",
    "        sr, audio = wavfile.read(audio_bytes)  # scipy로 샘플링 레이트 확인\n",
    "        audio_librosa, sr_librosa = librosa.load(audio_bytes, sr=None)  # librosa로 리샘플링\n",
    "        mfcc = librosa.feature.mfcc(y=audio_librosa, sr=sr_librosa, n_mfcc=13)  # MFCC 추출\n",
    "        mfcc_mean = np.mean(mfcc, axis=1).astype(float)  # 평균 계산\n",
    "        return mfcc_mean.tolist()  # 리스트로 반환\n",
    "    except Exception as e:\n",
    "        return None  # 에러 발생 시 None 반환\n",
    "\n",
    "# UDF 등록\n",
    "mfcc_udf = udf(extract_mfcc, ArrayType(FloatType()))\n",
    "\n",
    "# 🔹 파일 이름 추출 UDF 정의\n",
    "def extract_filename(path):\n",
    "    return os.path.basename(path)\n",
    "\n",
    "filename_udf = udf(extract_filename, StringType())\n",
    "\n",
    "# 🔹 변환 적용\n",
    "df_mfcc = binary_df \\\n",
    "    .withColumn(\"fileName\", filename_udf(binary_df[\"path\"])) \\\n",
    "    .withColumn(\"mfcc_features\", mfcc_udf(binary_df[\"content\"]))\n",
    "\n",
    "# 🔹 배열 데이터를 개별 컬럼으로 변환\n",
    "mfcc_columns = [f\"mfcc_{i+1}\" for i in range(13)]\n",
    "for i in range(13):\n",
    "    df_mfcc = df_mfcc.withColumn(mfcc_columns[i], df_mfcc[\"mfcc_features\"][i])\n",
    "\n",
    "# 🔹 불필요한 컬럼 정리\n",
    "df_mfcc = df_mfcc.select([\"fileName\"] + mfcc_columns)\n",
    "\n",
    "# 🔹 결과 저장 (HDFS)\n",
    "# output_path = \"hdfs://localhost:9000/shared_data/mfcc_features/\"\n",
    "# df_mfcc.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# print(f\"✅ MFCC 데이터가 HDFS에 저장됨: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a498916",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mfcc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d9df333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 MySQL로 성공적으로 적재되었습니다!\n"
     ]
    }
   ],
   "source": [
    "df_mfcc.createOrReplaceTempView(\"wav_motor_horn\")\n",
    "\n",
    "# SQL 쿼리 실행하여 데이터 추출\n",
    "df_mfcc = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM wav_motor_horn\n",
    "\"\"\")\n",
    "\n",
    "# MySQL연결\n",
    "mysql_url = \"jdbc:mysql://15.168.145.74:3306/my_db?useUnicode=true&characterEncoding=UTF-8\"\n",
    "mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# MySQL로 DataFrame 적재 (쿼리 결과가 None이 아닌 경우에만)\n",
    "if df_mfcc is not None:\n",
    "    df_mfcc.write.jdbc(url=mysql_url, table=\"wav_motorcycle_horn_data\", mode=\"overwrite\", properties=mysql_properties)\n",
    "    print(\"데이터가 MySQL로 성공적으로 적재되었습니다!\")\n",
    "else:\n",
    "    print(\"쿼리 결과가 없습니다. 데이터 추출이 실패했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116729f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb7f84b3",
   "metadata": {},
   "source": [
    "## 2. wav_motor_driving_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8d2392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import io\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StringType\n",
    "\n",
    "# HDFS에서 모든 WAV 파일 읽기\n",
    "hdfs_dir = \"hdfs://localhost:9000/shared_data/raw_data/2.Motorcycle/5.driving_sound_of_motorcycle\"\n",
    "binary_df = spark.read.format(\"binaryFile\").load(hdfs_dir)\n",
    "\n",
    "# 🔹 UDF (User Defined Function) 정의: WAV → MFCC 변환\n",
    "def extract_mfcc(binary_data):\n",
    "    try:\n",
    "        audio_bytes = io.BytesIO(binary_data)  # 바이너리 데이터를 메모리 파일로 변환\n",
    "        sr, audio = wavfile.read(audio_bytes)  # scipy로 샘플링 레이트 확인\n",
    "        audio_librosa, sr_librosa = librosa.load(audio_bytes, sr=None)  # librosa로 리샘플링\n",
    "        mfcc = librosa.feature.mfcc(y=audio_librosa, sr=sr_librosa, n_mfcc=13)  # MFCC 추출\n",
    "        mfcc_mean = np.mean(mfcc, axis=1).astype(float)  # 평균 계산\n",
    "        return mfcc_mean.tolist()  # 리스트로 반환\n",
    "    except Exception as e:\n",
    "        return None  # 에러 발생 시 None 반환\n",
    "\n",
    "# UDF 등록\n",
    "mfcc_udf = udf(extract_mfcc, ArrayType(FloatType()))\n",
    "\n",
    "# 🔹 파일 이름 추출 UDF 정의\n",
    "def extract_filename(path):\n",
    "    return os.path.basename(path)\n",
    "\n",
    "filename_udf = udf(extract_filename, StringType())\n",
    "\n",
    "# 🔹 변환 적용\n",
    "df_mfcc = binary_df \\\n",
    "    .withColumn(\"fileName\", filename_udf(binary_df[\"path\"])) \\\n",
    "    .withColumn(\"mfcc_features\", mfcc_udf(binary_df[\"content\"]))\n",
    "\n",
    "# 🔹 배열 데이터를 개별 컬럼으로 변환\n",
    "mfcc_columns = [f\"mfcc_{i+1}\" for i in range(13)]\n",
    "for i in range(13):\n",
    "    df_mfcc = df_mfcc.withColumn(mfcc_columns[i], df_mfcc[\"mfcc_features\"][i])\n",
    "\n",
    "# 🔹 불필요한 컬럼 정리\n",
    "df_mfcc = df_mfcc.select([\"fileName\"] + mfcc_columns)\n",
    "\n",
    "# 🔹 결과 저장 (HDFS)\n",
    "# output_path = \"hdfs://localhost:9000/shared_data/mfcc_features/\"\n",
    "# df_mfcc.write.csv(output_path, header=True, mode=\"overwrite\")\n",
    "\n",
    "# print(f\"✅ MFCC 데이터가 HDFS에 저장됨: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517239d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4735"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_mfcc.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86c2a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터가 MySQL로 성공적으로 적재되었습니다!\n"
     ]
    }
   ],
   "source": [
    "df_mfcc.createOrReplaceTempView(\"wav_motor_driving\")\n",
    "\n",
    "# SQL 쿼리 실행하여 데이터 추출\n",
    "df_mfcc = spark.sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM wav_motor_driving\n",
    "\"\"\")\n",
    "\n",
    "# MySQL연결\n",
    "mysql_url = \"jdbc:mysql://15.168.145.74:3306/my_db?useUnicode=true&characterEncoding=UTF-8\"\n",
    "mysql_properties = {\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"root\",\n",
    "    \"driver\": \"com.mysql.cj.jdbc.Driver\"\n",
    "}\n",
    "\n",
    "# MySQL로 DataFrame 적재 (쿼리 결과가 None이 아닌 경우에만)\n",
    "if df_mfcc is not None:\n",
    "    df_mfcc.write.jdbc(url=mysql_url, table=\"wav_motorcycle_driving_data\", mode=\"overwrite\", properties=mysql_properties)\n",
    "    print(\"데이터가 MySQL로 성공적으로 적재되었습니다!\")\n",
    "else:\n",
    "    print(\"쿼리 결과가 없습니다. 데이터 추출이 실패했습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70aec751",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ml_env_python3.6)",
   "language": "python",
   "name": "ml_env_python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
